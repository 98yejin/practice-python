# Practice Python ğŸ‘©â€ğŸ’»

## Contents

- Datastructures
- Algorithms
- Concurrent Programming
- Python Concurrency and Parallelism Problems
  - Concurrent Web Scraper
  - Parallel File Processing
  - Concurrent Producer-Consumer
- FAQs

## Datastructures

| index | datastructure | link                                 |
| ----- | ------------- | ------------------------------------ |
| 1     | Queue         | [link](/datastructure/_queue.py)     |
| 2     | Hashtable     | [link](/datastructure/hashtable.py)  |
| 3     | Linked list   | [link](/datastructure/linkedlist.py) |
| 4     | Stack         | [link](/datastructure/stack.py)      |
| 5     | Tree          | [link](/datastructure/tree.py)       |
| 6     | Minheap       | [link](/datastructure/minheap.py)    |
| 7     | trie          | [link](/datastructure/trie.py)       |
| 8     | merge sort    | [link](/datastructure/mergesort.py)  |

### Linked Lists

#### Singly Linked List

[Time Complexity]

- Accessing by index : O(m)
- Insertion/Deletion
- Head : O(1)
- Tail : O(n)
- Search: O(n)

[Space Complexity]

- One Node : O(1)
- Overall Node : O(n)

[When to use]

- Stacks, Queues
- Applications where memory efficiency is crucial.
- Situations where only forward traversal is required, such as parsing a linked list of data and processing it sequentially.

#### Doubly Linked List

[Time Complexity]

- Accessing by index : O(m)
- Insertion/Deletion
- Head : O(1)
- Tail : O(1)
- Search: O(n)

[Space Complexity]

- One Node : O(1)
- Overall Node : O(n)

[Why to use]

- Double linked list allow bidirectional traversal, which can be useful in certain scenarios, while single linked lists only support forward traversal.
- But consume more memory per node due to the additional reference to the previous node.

[When to use]

- Queues, deques, and hash tables
- Undo-redo functionality in text editors
- Applications where efficient insertion and deletion operations are required at both ends of the list
- Situations where bidirectional traversal is necessary, such as implementing a cursor in a text editor or navigating through a playlist in a media player

### Tree

- interview tip : íŠ¸ë¦¬ ë° ê·¸ë˜í”„ ë¬¸ì œë“¤ì€ ëŒ€ë¶€ë¶„ ì„¸ë¶€ì‚¬í•­ì´ ëª¨í˜¸í•˜ê±°ë‚˜ ê°€ì • ìì²´ê°€ í‹€ë¦° ê²½ìš°ê°€ ë§ë‹¤. ìœ ì˜í•˜ê³ , í•„ìš”í•˜ë‹¤ë©´ ëª…í™•í•˜ê²Œ í•´ ì¤„ ê²ƒì„ ìš”êµ¬í•˜ì.

[Traverse binary tree]

- In-order traversal ( `L -> C -> R` )
- Pre-order-traversal ( `C -> L -> R` )
- Post-order traversal ( `L -> R -> C` )

[Min-heap]

- íŠ¸ë¦¬ì˜ ë§ˆì§€ë§‰ ë‹¨ê³„ì—ì„œ ì˜¤ë¥¸ìª½ ë¶€ë¶„ì„ ëº€ ë¶€ë¶„ì´ ê°€ë“ ì±„ì›Œì¡Œë‹¤ëŠ” ì ì´ ì™„ì „ ì´ì§„ íŠ¸ë¦¬
- ê° ë…¸ë“œì˜ ì›ì†Œê°€ ìì‹ë³´ë‹¤ ì‘ë‹¤ëŠ” íŠ¹ì„±ì´ ìˆìŒ
- Insert( `O( logn)` ), extract_min ( `O( logn)` )

[When to use]

- Searching
- Tree operations
- Expression evaluation
- Serialization/deserialization
- Tree visualization

### Tries

[Time Complexity]

- The time complexity of operations in a Trie depends primarily on the length of the keys and the number of keys stored in the Trie.
- For operations like insertion, search, deletion, and prefix search, the time complexity is typically O(m), where m is the length of the keys involved in the operation.

[When to use]

- í•´ì‹œí…Œì´ë¸”ì„ ì´ìš©í•˜ë©´ ì£¼ì–´ì§„ ë¬¸ìì—´ì´ ìœ íš¨í•œì§€ ì•„ë‹Œì§€ ë¹ ë¥´ê²Œ í™•ì¸í•  ìˆ˜ ìˆì§€ë§Œ, ê·¸ ë¬¸ìì—´ì´ ì–´ë–¤ ìœ íš¨í•œ ë‹¨ì–´ì˜ ì ‘ë‘ì‚¬ì¸ì§€ í™•ì¸í•  ìˆ˜ëŠ” ì—†ë‹¤. íŠ¸ë¼ì´ë¥¼ ì´ìš©í•˜ë©´ ì•„ì£¼ ë¹ ë¥´ê²Œ í™•ì¸í•  ìˆ˜ ìˆë‹¤.
- íŠ¸ë¼ì´ëŠ” ê¸¸ì´ê°€ kì¸ ë¬¸ìì—´ì´ ì£¼ì–´ì¡Œì„ ë•Œ O(k) ì‹œê°„ì— í•´ë‹¹ ë¬¸ìì—´ì´ ìœ íš¨í•œ ì ‘ë‘ì‚¬ì¸ì§€ í™•ì¸í•  ìˆ˜ ìˆë‹¤.
- í•´ì‹œí…Œì´ë¸”ì„ ì‚¬ìš©í–ˆì„ ë•Œì™€ ê°™ì€ ìˆ˜í–‰ ì‹œê°„ì´ë‹¤.

[ETC]

- Prefix tree ë¼ê³ ë„ ë¶ˆë¦¬ëŠ” trie
- N-ary tree ì˜ ë³€ì¢…ìœ¼ë¡œ ê° ë…¸ë“œì— ë¬¸ìë¥¼ ì €ì¥í•˜ëŠ” ìë£Œêµ¬ì¡°
- íŠ¸ë¦¬ë¥¼ ì•„ë˜ìª½ìœ¼ë¡œ ìˆœíšŒí•˜ë©´ ë‹¨ì–´ í•˜ë‚˜ê°€ ë‚˜ì˜´
- ë„ ë…¸ë“œë¼ê³  ë¶ˆë¦¬ëŠ” \* ë…¸ë“œëŠ” ì¢…ì¢… ë‹¨ì–´ì˜ ëì„ ë‚˜íƒ€ëƒ„
- ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ì—ì„œ ë¬¸ìì—´ íƒìƒ‰ì„ ìœ„í•œ ìë£Œêµ¬ì¡°ë¡œ ë„ë¦¬ ì“°ì„
- ê°ê°ì˜ ë¬¸ì ë‹¨ìœ„ë¡œ ìƒ‰ì¸ì„ êµ¬ì¶•í•œ ê²ƒê³¼ ìœ ì‚¬

### Graph

- íŠ¸ë¦¬ëŠ” ì‚¬ì´í´ì´ ì—†ëŠ” í•˜ë‚˜ì˜ ì—°ê²° ê·¸ë˜í”„ì„.
- ê·¸ë˜í”„ê°€ íŠ¸ë¦¬ë³´ë‹¤ í° ê°œë…ì„.
- ê·¸ë˜í”„ëŠ” ë‹¨ìˆœíˆ ë…¸ë“œì™€ ê·¸ ë…¸ë“œë¥¼ ì—°ê²°í•˜ëŠ” ê°„ì„ (edge)ì„ í•˜ë‚˜ë¡œ ëª¨ì•„ë†“ì€ ê²ƒì„
- ë°©í–¥ì„±ì´ ìˆì„ ìˆ˜ë„ ìˆê³  ì—†ì„ ìˆ˜ë„ ìˆìŒ.
- ì‚¬ì´í´ì´ ì¡´ì¬í•˜ì§€ ì•Šì„ ìˆ˜ë„ ìˆê³  acycle graph ë¼ê³  ë¶€ë¦„
- ê·¸ë˜í”„ëŠ” ì—¬ëŸ¬ê°œì˜ ê³ ë¦½ëœ ë¶€ë¶„ ê·¸ë˜í”„(isolated subgraphs)ë¡œ êµ¬ì„±ë  ìˆ˜ ìˆê³ , ëª¨ë“  ì •ì  ìŒ(pair of vertices) ê°„ì— ê²½ë¡œê°€ ì¡´ì¬í•˜ëŠ” ê·¸ë˜í”„ëŠ” ì—°ê²° ê·¸ë˜í”„ë¼ê³  ë¶€ë¦„.

[ê·¸ë˜í”„ íƒìƒ‰]

- Depth-first search ( ëª¨ë“  ë…¸ë“œë¥¼ ë°©ë¬¸í•˜ê³ ì í•  ë•Œ ì„ í˜¸ë¨, ë°˜ë“œì‹œ ì–´ë–¤ ë…¸ë“œë¥¼ ë°©ë¬¸í–ˆëŠ”ì§€ ì²´í¬í•´ì•¼í•¨. ì•ˆê·¸ëŸ¬ë©´ ë¬´í•œë£¨í”„ ë¹ ì§€ê¸° ì‰¬ì›€ )
- Breadth-first search ( ìµœë‹¨ ê²½ë¡œ ë˜ëŠ” ì„ì˜ì˜ ê²½ë¡œ, Queue ì‚¬ìš© )

### Stack

[Time Complexity]

- empty() O(1)
- size(): O(1)
- top(), peek(): O(1)
- push(item): O1
- pop(): O(1)

[Why to use]

- ì¬ê·€ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•  ë•Œ ìœ ìš©í•˜ë‹¤.
- LIFO ìˆœì„œë¥¼ ë”°ë¥¸ë‹¤.
- Suitable when you need to access the most recently added item first or when the order of processing needs to be reversed.

[When to use]

- Function Call Management
- Expression Evaluation
- Undo/Redo Operations
- Backtracking Algorithms ( Depth First Search )
- Parsing Algorithms like XML/HTML

### Queue

[Time Complexity]

- enqueue(item):O(1)
- dequeue(): O(1)
- peek(): O(1)
- isEmpty(): O(1)

Pythonâ€™s collections.deque

- Copy O(n)
- Append, Append left O(1)
- Pop, Pop left O (1)
- Extend, extend left O(k)
- Rotate O(k)
- Remove O(n)
- Get Length O(1)

[Why to use]

- Suitable when you need to process items in the order they were added or when implementing systems with asynchronous tasks or requests.
- Queues are commonly used in systems with asynchronous tasks or requests, where tasks/requests arrive at irregular intervals and need to be processed in the order they were received. For example, in web servers, incoming HTTP requests are often placed in a queue and processed by worker threads in the order they were received.

[When to use]

- Job Scheduling
- Breadth First Search
- Buffering
- Print Queue Management
- Bounded Buffer

### Heapq ( priority queue )

[methods]

- heapq.heappush()
- heapq.heappop()
- heapq.heapify()
- heapq.nlargest()
- heapq.nsmallest()

[when to use]

- Heapsort
- Graph Algorithm
  (Dijkstra, Prim, A\*)
- File Compress
- Load Balancing

[ETC]

- Heaps are binary trees for which every parent node has a value less than or equal to any of its children. We refer to this condition as the heap invariant.
- heap[k] <= heap[2*k+1] or heap[k] <= heap[2\*k+2

### List

[Time Complexity]

- Copy O(n)
- Append O(1)
- Pop Last O(1)
- Pop intermediate O(n)
- Insert O(n)
- Get/Set Item O(1)
- Delete Item O(n)
- Iteration O(n)
- Get Slice O(k)
- Del Slice O(n)
- Set Slice O(k+n)
- Extend O(k)
- Sort O(nlogn)
- Multiply O(nk)
- x in s O(n)
- min(s): O(n)

[When to use]

- Dynamic memory allocation ( scenarios where the size of the data collection is not known in advance )
- Implementing stacks and queues

[Etc]

- ë°°ì—´ì˜ í¬ê¸°ë¥¼ ìë™ìœ¼ë¡œ ì¡°ì ˆí•  ìˆ˜ ìˆìŒ.
- ë°ì´í„°ë¥¼ ë§ë¶™ì¼ ë•Œë§ˆë‹¤ ë°°ì—´ í˜¹ì€ ë¦¬ìŠ¤íŠ¸ì˜ í¬ê¸°ê°€ ì¦ê°€í•¨.
- í•„ìš”ì— ë”°ë¼ í¬ê¸°ë¥¼ ë³€í™”ì‹œí‚¬ ìˆ˜ ìˆìœ¼ë©´ì„œë„ O(1)ì˜ ì ‘ê·¼ ì‹œê°„ì„ ìœ ì§€í•¨. í†µìƒì ìœ¼ë¡œ ë°°ì—´ì´ ê°€ë“ ì°¨ëŠ” ìˆœê°„, ë°°ì—´ì˜ í¬ê¸°ë¥¼ ë‘ ë°°ë¡œ ëŠ˜ë¦¼. í¬ê¸°ë¥¼ ë‘ ë°° ëŠ˜ë¦¬ëŠ” ì‹œê°„ì€ O(n)ì´ì§€ë§Œ ìì£¼ ë°œìƒí•˜ëŠ” ì¼ì´ ì•„ë‹ˆì–´ì„œ amortized insertion time ìœ¼ë¡œ ê³„ì‚°í–ˆì„ ë•ŒëŠ” ì—¬ì „íˆ O(1) ì„

### Hash Table

[Time Complexity]

- K in d O(1) / O(n)
- Copy O(n) / O(n)
- get item O(1) / O(n)
- Set Item O(1) / O(n)
- Delete item O(1) / O(n)
- Iteration O(n) / O(n)

Avg case times listed for dict objects assume that hash function for the objects is sufficiently robust to make collisions uncommon

[Why to use]

- íš¨ìœ¨ì ì¸ íƒìƒ‰ì„ ìœ„í•œ ìë£Œêµ¬ì¡°
- Linked list ì™€ hash code function ìœ¼ë¡œ êµ¬í˜„í•  ìˆ˜ ìˆìŒ
- ë˜ëŠ” ê· í˜• ì´ì§„ íƒìƒ‰ íŠ¸ë¦¬ë¡œ êµ¬í˜„í•  ìˆ˜ ìˆìŒ. í¬ê¸°ê°€ í° ë°°ì—´ì„ ë¯¸ë¦¬ í• ë‹¹í•´ ë†“ì§€ ì•Šì•„ë„ ë˜ê¸° ë•Œë¬¸ì— ì ì¬ì ìœ¼ë¡œ ì ì€ ê³µê°„ì„ ì‚¬ìš©í•¨.
- í‚¤ì˜ì§‘í•©ì„ íŠ¹ì • ìˆœì„œëŒ€ë¡œ ì ‘ê·¼í•  ìˆ˜ ìˆìŒ

[When to use]

- Storing key-value pairs
- Database indexing
- Caching
- Symbol tables in compilers
- Implementing sets and bags
- Counting frequencies
- Hash-based algorithms

## Algorithms

| index | algorithm     | link                                |
| ----- | ------------- | ----------------------------------- |
| 1     | BFS           | [link](/algorithms/bfs.py)          |
| 2     | DFS           | [link](/algorithms/dfs.py)          |
| 3     | Binary Search | [link](/algorithms/binarysearch.py) |
| 4     | Quick Sort    | [link](/algorithms/quicksort.py)    |

### Breadth-First Search (BFS)

[Time Complexity]

O(V + E), where V is the number of vertices (nodes) and E is the number of edges in the graph.

[Space Complexity]

O(V), where V is the number of vertices. In the worst case, the entire graph may need to be stored in the queue.

[When to use]

BFS is used to explore the vertices of a graph level by level, starting from a given source vertex. It is often used to find the shortest path in unweighted graphs, connected components, and more.

### Depth-First Search (DFS)

[Time Complexity]

O(V + E), where V is the number of vertices (nodes) and E is the number of edges in the graph. However, DFS can be faster in practice for some graph structures.

[Space Complexity]

O(V), where V is the number of vertices. In the worst case, the entire graph may need to be stored in the call stack.

[When to use]

DFS is used to explore the vertices of a graph deeply, following each branch until it reaches a dead end before backtracking. It is often used for topological sorting, cycle detection, maze solving, and more.

### Binary Search

[Time Complexity]

- O(log n), where n is the number of elements in the sorted array.
- if the array is not sorted, binary search cannot reliably find the target element. In such cases, alternative methods such as linear search (iterating through the array sequentially) may be used to find the target element, but with a time complexity of O(n) instead of O(log n) provided by binary search on a sorted array.

[Space Complexity]

O(1). Binary search is an in-place algorithm and does not require additional space.

[When to use]

Binary search is used to find the position of a target value within a sorted array efficiently. It repeatedly divides the search interval in half until the target value is found or the interval becomes empty.

### Merge Sort

[Time Complexity]

O(n log n), where n is the number of elements in the array.

[Space Complexity]

O(n), where n is the number of elements in the array. Merge sort requires additional space for merging subarrays during the merge step.

[How it works]

- Merge sort is a divide-and-conquer algorithm that recursively divides the array into smaller subarrays, sorts them independently, and then merges the sorted subarrays to produce the final sorted array.
- It is a stable sorting algorithm and is often used for sorting large datasets efficiently.

[Why to use]

- íš¨ìœ¨ì„± : ë³‘í•© ì •ë ¬ì€ ëª¨ë“  ê²½ìš°ì— O(n log n)ì˜ ì‹œê°„ ë³µì¡ë„ë¥¼ ê°€ì§€ë¯€ë¡œ ëŒ€ê·œëª¨ ë°ì´í„° ì„¸íŠ¸ë¥¼ ì •ë ¬í•˜ëŠ” ë° íš¨ìœ¨ì ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ì •ë ¬ ì„±ëŠ¥ì´ ì¤‘ìš”í•œ ì‘ìš© ë¶„ì•¼ì— ì í•©í•©ë‹ˆë‹¤.
- ì•ˆì •ì„± : ë³‘í•© ì •ë ¬ì€ ì•ˆì •ì ì¸ ì •ë ¬ ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤. ì¦‰, ë™ì¼í•œ ìš”ì†Œì˜ ìƒëŒ€ì  ìˆœì„œë¥¼ ìœ ì§€í•©ë‹ˆë‹¤. ì´ ì†ì„±ì€ ë™ì¼í•œ ìš”ì†Œì˜ ì›ë˜ ìˆœì„œë¥¼ ìœ ì§€í•´ì•¼ í•˜ëŠ” ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ì¤‘ìš”í•©ë‹ˆë‹¤.
- ë¶„í•  ì •ë³µ : ë³‘í•© ì •ë ¬ì€ ì •ë ¬ ë¬¸ì œë¥¼ ë” ì‘ì€ í•˜ìœ„ ë¬¸ì œë¡œ ë‚˜ëˆ„ê³  ë…ë¦½ì ìœ¼ë¡œ ì •ë ¬í•œ ë‹¤ìŒ ì •ë ¬ëœ í•˜ìœ„ ë°°ì—´ì„ ë³‘í•©í•˜ëŠ” ë¶„í•  ì •ë³µ ì „ëµì„ ë”°ë¦…ë‹ˆë‹¤. ì´ëŠ” ë‹¤ë¥¸ ì •ë ¬ ì•Œê³ ë¦¬ì¦˜ì— ë¹„í•´ êµ¬í˜„í•˜ê³  ì´í•´í•˜ê¸°ê°€ ë” ì‰½ìŠµë‹ˆë‹¤.
- ì˜ˆì¸¡ ê°€ëŠ¥í•œ ì„±ëŠ¥ : ë³‘í•© ì •ë ¬ì€ ì¼ê´€ëœ ì„±ëŠ¥ íŠ¹ì„±ì„ ê°€ì§€ë©° ë‹¤ë¥¸ ì •ë ¬ ì•Œê³ ë¦¬ì¦˜ê³¼ ê°™ì´ ìµœì•…ì˜ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ê²ªì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ í¬ê¸°ì™€ ë¶„í¬ë¥¼ ê°€ì§„ ë°ì´í„° ì„¸íŠ¸ì—ì„œë„ ì˜ ì‘ë™í•©ë‹ˆë‹¤.
- ë‚´ë¶€ ì•„ë‹˜(Not In-Place) : ë³‘í•© ì •ë ¬ì€ ë‚´ë¶€ ì •ë ¬ ì•Œê³ ë¦¬ì¦˜ì´ ì•„ë‹ˆë©° í•˜ìœ„ ë°°ì—´ ë³‘í•©ì„ ìœ„í•´ ì¶”ê°€ ë©”ëª¨ë¦¬ê°€ í•„ìš”í•˜ì§€ë§Œ íš¨ìœ¨ì„±ê³¼ ì•ˆì •ì„± ì´ì ì´ ì´ ì˜¤ë²„í—¤ë“œë³´ë‹¤ ë” í° ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.

### Quick Sort

[Time Complexity]

O(n log n) on average, O(n^2) in the worst case, where n is the number of elements in the array.

[Space Complexity]

O(log n) on average, O(n) in the worst case due to recursive calls. Quick sort is an in-place sorting algorithm and does not require additional space for sorting.

[How it works]

- Quick sort is a divide-and-conquer algorithm that recursively partitions the array into smaller subarrays based on a pivot element, sorts the subarrays independently, and then combines them to produce the final sorted array.
- It is widely used in practice due to its efficiency on average and ease of implementation.

## Concurrent Programming

| index | concurrent programming  | link                                           |
| ----- | ----------------------- | ---------------------------------------------- |
| 1     | barriers                | [link](/concurrent/barriers.py)                |
| 2     | fizzbuzz                | [link](/concurrent/fizzbuzz.py)                |
| 3     | fizzbuzz_async          | [link](/concurrent/fizzbuzz_async.py)          |
| 4     | fizzbuzz_multithreading | [link](/concurrent/fizzbuzz_multithreading.py) |
| 5     | future                  | [link](/concurrent/future.py)                  |
| 6     | helloworld              | [link](/concurrent/helloworld.py)              |
| 7     | interface               | [link](/concurrent/interface.py)               |
| 8     | locks                   | [link](/concurrent/locks.py)                   |
| 9     | quickstart              | [link](/concurrent/quickstart.py)              |
| 10    | semaphores              | [link](/concurrent/semaphores.py)              |
| 11    | sharedresources         | [link](/concurrent/sharedresources.py)         |

### Common Concurrency Bugs

- Race Conditions: Occur when multiple threads access shared data concurrently, leading to unexpected behavior.
- Deadlocks: Happen when two or more threads are stuck waiting for each other to release resources, resulting in a deadlock.
- Livelocks: Similar to deadlocks, but threads are actively performing operations without making progress.
- Resource Leaks: Occur when resources (e.g., locks, semaphores) are not properly released, leading to resource exhaustion.

### Concurrency Concepts

- Parallelism vs. Concurrency:
  - Parallelism involves executing multiple tasks simultaneously on different processors or cores.
  - Concurrency involves managing and coordinating multiple tasks, which may or may not execute simultaneously.
- Processes and Threads:
  - Processes are separate instances of a program, each with its own memory space.
  - Threads are lightweight units of execution within a process, sharing the same memory space.
- Shared Resources and Synchronization:
  - Concurrent tasks often access shared resources, such as memory or files.
  - Synchronization mechanisms, like locks and semaphores, are used to coordinate access to shared resources and prevent race conditions.
- Communication and Coordination:
  - Concurrent tasks may need to communicate and coordinate with each other.
  - Inter-process communication (IPC) mechanisms, like pipes and message queues, enable communication between processes.
  - Inter-thread communication can be achieved through shared memory and synchronization primitives.

### Trade-offs

1. Complexity:
   - Concurrent programs are generally more complex to design, implement, and debug compared to sequential programs.
   - Coordinating and synchronizing concurrent tasks adds an additional layer of complexity.
2. Overhead:
   - Creating and managing concurrent units of execution (processes or threads) introduces overhead.
   - Context switching between tasks and synchronization operations can have performance implications.
3. Scalability:
   - Concurrency can improve the scalability of a program by allowing it to utilize available system resources effectively.
   - However, the scalability benefits may diminish if there are excessive synchronization or communication overheads.
4. Debugging and Testing:
   - Debugging concurrent programs can be challenging due to the non-deterministic nature of task interleaving.
   - Race conditions and deadlocks can be difficult to reproduce and diagnose.
   - Testing concurrent programs requires careful consideration of various interleaving scenarios.
5. Resource Utilization:
   - Concurrent programs can make efficient use of system resources, such as CPU cores and I/O devices.
   - However, excessive concurrency can lead to resource contention and decreased performance.
6. Synchronization Granularity:
   - Fine-grained synchronization (frequent locking) can minimize the time tasks spend waiting but introduces more overhead.
   - Coarse-grained synchronization (less frequent locking) reduces overhead but may lead to reduced concurrency.
7. Deadlocks and Livelocks:
   - Improper synchronization can result in deadlocks, where tasks are stuck waiting for each other indefinitely.
   - Livelocks occur when tasks are actively performing operations but unable to make progress.

### Processes, Threads, and the Global Interpreter Lock (GIL) in Python

- Processes:

  - Processes are separate instances of a program, each with its own memory space and system resources.
  - They provide true parallelism by running on separate CPU cores or even across multiple machines.
  - Processes have their own Python interpreter and do not share memory by default.
  - Communication between processes is done through inter-process communication (IPC) mechanisms like pipes, queues, or shared memory.

- Threads:
  - Threads are lightweight units of execution within a process.
  - They share the same memory space and can access the same global variables and data structures.
  - Threads are managed by the Python interpreter and are subject to the Global Interpreter Lock (GIL).
  - They are suitable for I/O-bound tasks or tasks that involve waiting for external resources.
- Global Interpreter Lock (GIL):
  - The GIL is a mutex that prevents multiple native threads from executing Python bytecodes simultaneously.
  - It ensures that only one thread can execute Python code at a time, even in a multi-threaded environment.
  - The GIL is necessary to protect the integrity of Python objects and manage memory safely.
  - It can limit the performance of CPU-bound tasks in multi-threaded Python programs.

### Differences between CPU-bound and I/O-bound tasks

- CPU-bound tasks:

  - These tasks are computationally intensive and spend most of their time utilizing the CPU.
  - Examples include complex calculations, data processing, or algorithmic operations.
  - CPU-bound tasks benefit from true parallelism, where multiple processes can execute on separate CPU cores simultaneously.
  - In Python, due to the GIL, multi-threading may not provide significant performance improvements for CPU-bound tasks.

- I/O-bound tasks:
  - These tasks spend most of their time waiting for input/output operations to complete.
  - Examples include reading from or writing to files, network communication, or interacting with databases.
  - I/O-bound tasks often involve blocking operations, where the program waits for the I/O operation to finish.
  - Multi-threading can be effective for I/O-bound tasks, as threads can perform other tasks while waiting for I/O operations to complete.

### Race Conditions, Deadlocks, and Synchronization Primitives

- Race Conditions:

  - A race condition occurs when multiple threads access shared data concurrently, and the final outcome depends on the relative timing of their execution.
  - It happens when threads perform non-atomic operations on shared data without proper synchronization.
  - Race conditions can lead to unpredictable and incorrect program behavior.
  - To prevent race conditions, synchronization mechanisms like locks or atomic operations should be used to ensure exclusive access to shared data.

- Deadlocks:
  - A deadlock is a situation where two or more threads are unable to proceed because each is waiting for the other to release a resource.
  - It occurs when there is a circular dependency between threads, and each thread is holding a resource that the other thread needs.
  - Deadlocks can result in a program becoming unresponsive or stuck indefinitely.
  - Techniques like resource ordering, timeout mechanisms, or deadlock detection algorithms can be used to prevent or recover from deadlocks.
- Synchronization Primitives:
  - Synchronization primitives are mechanisms used to coordinate the execution of concurrent threads and prevent race conditions and deadlocks.
  - Locks (mutexes) are used to ensure exclusive access to shared resources, allowing only one thread to execute a critical section at a time.
  - Semaphores are used to control access to a limited number of resources, allowing multiple threads to access the resources up to a specified limit.
  - Condition variables are used to synchronize threads based on a specific condition, allowing threads to wait until the condition is met.
  - Barriers are used to synchronize a group of threads, ensuring that all threads reach a certain point before proceeding further.

## Python Concurrency and Parallelism Problems

### [Concurrent Web Scraper](/crawler/)

- Objective: Implement a concurrent web scraper that retrieves data from
  multiple web pages simultaneously.
- Requirements:
  - Use the requests library to fetch web pages.
  - Utilize the asyncio module to achieve concurrency.
  - Scrape data from a list of URLs concurrently.
  - Process and store the scraped data.
- Extension: Implement rate limiting to avoid overloading the target website.

### [Parallel File Processing](/file_processing/)

- Objective: Develop a program that processes a large set of files concurrently.
- Requirements:
  - Accept a directory path as input.
  - Use the multiprocessing module to process files concurrently.
  - Apply a specific operation to each file (e.g., compression, encryption, or
    analysis).
  - Display the progress and status of the file processing.
- Extension: Implement error handling and logging for file processing failures.

### [Concurrent Producer-Consumer](/producer_consumer/)

- Objective: Implement a concurrent producer-consumer system using threads.
- Requirements:
  - Create a shared queue to store data items.
  - Implement producer threads that generate and add data items to the queue.
  - Implement consumer threads that retrieve and process data items from the
    queue.
  - Use synchronization primitives (e.g., locks or semaphores) to ensure thread
    safety.
  - Display the production and consumption of data items.
- Extension: Implement a mechanism to gracefully terminate the producer and
  consumer threads.

## FAQs

### Memory - Heap vs Stack

#### Stack

- Automatic Memory Management: The stack is managed automatically by the compiler or runtime environment. Memory allocated on the stack is automatically deallocated when it goes out of scope.
- Static Memory Allocation: Memory allocation on the stack follows a Last-In-First-Out (LIFO) strategy. Each function call creates a new stack frame, which includes parameters, local variables, and return addresses.
  Limited Size: The stack has a fixed and limited size, typically smaller than the heap. This size is determined at compile time or by system settings.
- Faster Access: Accessing memory on the stack is faster than on the heap because it involves simple pointer arithmetic.
- Scope-based: Variables allocated on the stack have a limited scope and are only accessible within the block or function in which they are declared.

#### Heap

- Manual Memory Management: Memory on the heap is managed manually by the programmer. Memory allocated on the heap must be explicitly deallocated to prevent memory leaks.
- Dynamic Memory Allocation: Memory allocation on the heap can be dynamic, allowing for flexible memory usage and allocation of memory blocks of varying sizes.
- Unlimited Size: The heap typically has a larger and theoretically unlimited size compared to the stack. However, it can be limited by available system resources.
- Slower Access: Accessing memory on the heap involves dereferencing pointers, which can be slower than accessing memory on the stack.
- Global Scope: Memory allocated on the heap can have a global scope and can be accessed from anywhere in the program.
  Potential for Memory Fragmentation: Memory fragmentation can occur on the heap over time, leading to inefficient memory usage and potential allocation failures.
- Choosing Between Stack and Heap:
  Lifetime of Data: Use the stack for data with a short lifetime, such as function parameters and local variables. Use the heap for data with a longer lifetime or dynamic memory requirements.
- Scope: Use the stack for variables with a limited scope and predictable lifetime. Use the heap for variables with a broader scope or lifetime extending beyond the current scope.
- Size Consideration: Consider the size of data when choosing between the stack and heap. Large data structures may exceed the stack's size limit and should be allocated on the heap.

In summary, the stack and heap serve different purposes in memory management, with the stack providing automatic and scoped memory allocation, while the heap offers manual and dynamic memory allocation. The choice between stack and heap depends on factors such as data lifetime, scope, and memory requirements.

### Recursion vs Iteration

ì¬ê·€ì  ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ë©´ ê³µê°„ íš¨ìœ¨ì„±ì´ ë‚˜ë¹ ì§ˆ ìˆ˜ ìˆë‹¤. ì¬ê·€ í˜¸ì¶œì´ í•œ ë²ˆ ë°œìƒí•  ë•Œë§ˆë‹¤ ìŠ¤íƒì— ìƒˆë¡œìš´ ì¸µ(layer)ì„ ì¶”ê°€í•´ì•¼ í•œë‹¤. ì´ëŠ” ì¬ê·€ì˜ ê¹Šì´ê°€ nì¼ ë•Œ O(n) ë§Œí¼ì˜ ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•˜ê²Œ ëœë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.
ì´ëŸ° ì´ìœ ë¡œ ì¬ê·€ì  ì•Œê³ ë¦¬ì¦˜ì„ ìˆœí™˜ì ìœ¼ë¡œ êµ¬í˜„í•˜ëŠ” ê²Œ ë” ë‚˜ì„ ìˆ˜ë„ ìˆë‹¤. í•˜ì§€ë§Œ ìˆœí™˜ì ìœ¼ë¡œ êµ¬í˜„ëœ ì½”ë“œëŠ” ë•Œë¡œëŠ” í›¨ì”¬ ë” ë³µì¡í•˜ê¸° ë•Œë¬¸ì— íƒ€í˜‘í•˜ëŠ” ê²Œ ë‚˜ì„ ìˆ˜ ìˆë‹¤.

```
python recursive íšŸìˆ˜ ê¸°ë³¸ì ìœ¼ë¡œ 1000ë²ˆìœ¼ë¡œ ì„¤ì •ë˜ì–´ ìˆìŒ. ë³€ê²½ì€ ê°€ëŠ¥í•˜ì§€ë§Œ, recursion error ê°€ ë°œìƒí•´ì„œ ìŠ¤íƒì´ í„°ì§ˆ ìˆ˜ ìˆê³  ì´ëŸ° ê²½ìš° iterative approach ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒë„ ë°©ë²•ì„
```

### Process vs Thread

- í”„ë¡œì„¸ìŠ¤ëŠ” ì‹¤í–‰ë˜ê³  ìˆëŠ” í”„ë¡œê·¸ë¨ì˜ ì¸ìŠ¤í„´ìŠ¤ë¼ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤. í”„ë¡œì„¸ìŠ¤ëŠ” CPU ì‹œê°„ì´ë‚˜ ë©”ëª¨ë¦¬ ë“±ì˜ ì‹œìŠ¤í…œ ìì›ì´ í• ë‹¹ë˜ëŠ” ë…ë¦½ì ì¸ ê°ì²´ì´ë‹¤. ê° í”„ë¡œì„¸ìŠ¤ëŠ” ë³„ë„ì˜ ì£¼ì†Œ ê³µê°„ì—ì„œ ì‹¤í–‰ë˜ë©°, í•œ í”„ë¡œì„¸ìŠ¤ëŠ” ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ì˜ ë³€ìˆ˜ë‚˜ ìë£Œêµ¬ì¡°ì— ì ‘ê·¼í•  ìˆ˜ ì—†ë‹¤. í•œ í”„ë¡œì„¸ìŠ¤ê°€ ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ì˜ ìì›ì— ì ‘ê·¼í•˜ë ¤ë©´ í”„ë¡œì„¸ìŠ¤ ê°„ í†µì‹ (IPC, inter-process communication)ì„ ì‚¬ìš©í•´ì•¼ í•œë‹¤. í”„ë¡œì„¸ìŠ¤ ê°„ í†µì‹  ë°©ë²•ìœ¼ë¡œëŠ” íŒŒì´í”„, íŒŒì¼, ì†Œì¼“(pipes, queues, or shared memory) ë“±ì„ ì´ìš©í•œ ë°©ë²•ì´ ìˆë‹¤.

- ìŠ¤ë ˆë“œëŠ” í”„ë¡œì„¸ìŠ¤ ì•ˆì— ì¡´ì¬í•˜ë©° í”„ë¡œì„¸ìŠ¤ì˜ ìì›(í™ ê³µê°„ ë“±)ì„ ê³µìœ í•œë‹¤. ê°™ì€ í”„ë¡œì„¸ìŠ¤ ì•ˆì— ìˆëŠ” ì—¬ëŸ¬ ìŠ¤ë ˆë“œë“¤ì€ ê°™ì€ í™ ê³µê°„ì„ ê³µìœ í•œë‹¤. ë°˜ë©´ì— í”„ë¡œì„¸ìŠ¤ëŠ” ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ì˜ ë©”ëª¨ë¦¬ì— ì§ì ‘ ì ‘ê·¼í•  ìˆ˜ ì—†ë‹¤. ê°ê°ì˜ ìŠ¤ë ˆë“œëŠ” ë³„ë„ì˜ ë ˆì§€ìŠ¤í„°ì™€ ìŠ¤íƒì„ ê°–ê³  ìˆì§€ë§Œ, í™ ë©”ëª¨ë¦¬ëŠ” ì„œë¡œ ì½ê³  ì“¸ ìˆ˜ ìˆë‹¤.

- ìŠ¤ë ˆë“œëŠ” í”„ë¡œì„¸ìŠ¤ì˜ íŠ¹ì •í•œ ìˆ˜í–‰ ê²½ë¡œì™€ ê°™ë‹¤. í•œ ìŠ¤ë ˆë“œê°€ í”„ë¡œì„¸ìŠ¤ ìì›ì„ ë³€ê²½í•˜ë©´, ë‹¤ë¥¸ ì´ì›ƒ ìŠ¤ë ˆë“œ(sibling thread)ë„ ê·¸ ë³€ê²½ ê²°ê³¼ë¥¼ ì¦‰ì‹œ ë³¼ ìˆ˜ ìˆë‹¤.

### Context Switching

Context Switching ì€ ë‘ í”„ë¡œì„¸ìŠ¤ë¥¼ ì „í™˜í•˜ëŠ” ë° ë“œëŠ” ì‹œê°„ì´ë‹¤. ì¦‰, ëŒ€ê¸° ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹¤í–‰ ìƒíƒœë¡œ ì „í™˜í•˜ê³ , ì‹¤í–‰ ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ë¥¼ ëŒ€ê¸° ìƒíƒœë‚˜ ì¢…ë£Œ ìƒíƒœë¡œ ì „í™˜í•˜ëŠ” ë° ë“œëŠ” ì‹œê°„ì´ë‹¤. ë©€í‹°íƒœìŠ¤í‚¹ì„ í•  ë•Œ ì´ëŸ° ì¼ì´ ë°œìƒí•œë‹¤. ìš´ì˜ì²´ì œëŠ” ëŒ€ê¸°ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ì˜ ìƒíƒœ ì •ë³´ë¥¼ ë©”ëª¨ë¦¬ì— ì˜¬ë¦¬ê³ , í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ì˜ ìƒíƒœ ì •ë³´ëŠ” ì €ì¥í•´ì•¼ í•œë‹¤.

### Deadlock

[ë°œìƒ ì¡°ê±´]

- Mutual Exclusion: í•œë²ˆì— í•œ í”„ë¡œì„¸ìŠ¤ë§Œ ê³µìœ  ìì›ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.
- Hold and Wait: ê³µìœ  ìì›ì— ëŒ€í•œ ì ‘ê·¼ ê¶Œí•œì„ ê°–ê³  ìˆëŠ” í”„ë¡œì„¸ìŠ¤ê°€, ê·¸ ì ‘ê·¼ ê¶Œí•œì„ ì–‘ë³´í•˜ì§€ ì•Šì€ ìƒíƒœì—ì„œ ë‹¤ë¥¸ ìì›ì— ëŒ€í•œ ì ‘ê·¼ ê¶Œí•œì„ ìš”êµ¬í•  ìˆ˜ ìˆë‹¤
- No Preemption: í•œ í”„ë¡œì„¸ìŠ¤ê°€ ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ì˜ ìì› ì ‘ê·¼ ê¶Œí•œì„ ê°•ì œë¡œ ì·¨ì†Œí•  ìˆ˜ ì—†ë‹¤
- Circular wait: ë‘ ê°œ ì´ìƒì˜ í”„ë¡œì„¸ìŠ¤ê°€ ìì› ì ‘ê·¼ì„ ê¸°ë‹¤ë¦¬ëŠ”ë°, ê·¸ ê´€ê³„ì— ì‚¬ì´í´ì´ ì¡´ì¬í•œë‹¤

ê³µìœ  ìì› ì¤‘ ë§ì€ ê²½ìš° Mutual Exclusion ì€ í•´ì œí•˜ê¸° ì–´ë µê¸° ë•Œë¬¸ì—, Circular Wait ì„ ë°©ì§€í•˜ëŠ” ë° ì´ˆì ì´ ë§ì¶°ì ¸ ìˆë‹¤.

### Synchroneous vs Asynchroneous

I/O ì‘ì—…ì´ ì—†ëŠ” ì´ìƒ GIL ë•Œë¬¸ì— ë™ê¸° í•¨ìˆ˜ê°€ í›¨ì”¬ ë¹ ë¥´ë‹¤.

- í•¨ìˆ˜ í˜¸ì¶œê³¼ ì˜¤ë²„í—¤ë“œ

  - ë™ê¸° í•¨ìˆ˜: ë™ê¸° í•¨ìˆ˜ëŠ” ì§ì ‘ì ì´ê³  ë‹¨ìˆœí•œ í˜¸ì¶œ ìŠ¤íƒì„ ê°€ì§‘ë‹ˆë‹¤. ê° í•¨ìˆ˜ëŠ” í˜¸ì¶œë˜ë©´ ì™„ë£Œë  ë•Œê¹Œì§€ ì‹¤í–‰ë˜ë©°, ë‹¤ë¥¸ ì‘ì—…ìœ¼ë¡œì˜ ì»¨í…ìŠ¤íŠ¸ ì „í™˜ì€ ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ëŠ” ë§¤ìš° íš¨ìœ¨ì ì´ë©°, ì‘ì—…ì˜ ë¶ˆí•„ìš”í•œ ì§€ì—°ì´ ì—†ìŠµë‹ˆë‹¤.
  - ë¹„ë™ê¸° í•¨ìˆ˜: asyncioì™€ ê°™ì€ ë¹„ë™ê¸° í•¨ìˆ˜ëŠ” ì´ë²¤íŠ¸ ë£¨í”„ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ì—…ì„ ìŠ¤ì¼€ì¤„ë§í•˜ê³  ì‹¤í–‰í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ í•¨ìˆ˜ í˜¸ì¶œ, ì‘ì—… ì¤€ë¹„, ì´ë²¤íŠ¸ ë£¨í”„ì— ì‘ì—…ì„ ë“±ë¡í•˜ê³ , ì™„ë£Œë¥¼ ê¸°ë‹¤ë¦¬ëŠ” ë“±ì˜ ì¶”ê°€ì ì¸ ìŠ¤í…ì´ í•„ìš”í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê° ìŠ¤í…ì€ ì˜¤ë²„í—¤ë“œë¥¼ ë°œìƒì‹œí‚¤ë©°, íŠ¹íˆ CPU ë°”ìš´ë“œ ì‘ì—…ì—ì„œëŠ” ì´ ì˜¤ë²„í—¤ë“œê°€ ì „ì²´ ì‹¤í–‰ ì‹œê°„ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- ì‘ì—…ì˜ ë³¸ì§ˆ

  - CPU ë°”ìš´ë“œ ì‘ì—…: FizzBuzzì™€ ê°™ì€ ë¬¸ì œëŠ” ìˆ«ìë¥¼ ê³„ì‚°í•˜ê³  ì¡°ê±´ì„ í‰ê°€í•˜ëŠ” CPU ë°”ìš´ë“œ ì‘ì—…ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ì‘ì—…ì—ì„œëŠ” ì—°ì†ì ì´ê³  ë¹ ë¥¸ ì²˜ë¦¬ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤. CPU ë°”ìš´ë“œ ì‘ì—…ì—ì„œëŠ” ì‘ì—…ì„ ë¹ ë¥´ê²Œ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ë™ê¸° ì²˜ë¦¬ ë°©ì‹ì´ ë” ì í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  - I/O ë°”ìš´ë“œ ì‘ì—…: ë¹„ë™ê¸° í•¨ìˆ˜ëŠ” I/O ì‘ì—…(ì˜ˆ: íŒŒì¼ ì½ê¸°/ì“°ê¸°, ë„¤íŠ¸ì›Œí¬ ìš”ì²­)ì„ ì²˜ë¦¬í•  ë•Œ íš¨ê³¼ì ì…ë‹ˆë‹¤. I/O ì‘ì—…ì€ ëŒ€ê¸° ì‹œê°„ì´ ê¸¸ê¸° ë•Œë¬¸ì—, ì´ë²¤íŠ¸ ë£¨í”„ëŠ” I/O ì‘ì—…ì´ ì™„ë£Œë˜ê¸°ë¥¼ ê¸°ë‹¤ë¦¬ëŠ” ë™ì•ˆ ë‹¤ë¥¸ ì‘ì—…ì„ ì§„í–‰í•  ìˆ˜ ìˆì–´ ìì›ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°ì˜ ì í•©ì„±
  - ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°ì˜ ì í•©ì„±: ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°ì€ ë³¸ì§ˆì ìœ¼ë¡œ ë¹„íš¨ìœ¨ì ì¸ I/O ì‘ì—…ì„ ìµœì í™”í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ I/O ë°”ìš´ë“œê°€ ì•„ë‹Œ ê°„ë‹¨í•œ ê³„ì‚° ì‘ì—…ì— ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°ì„ ì ìš©í•˜ëŠ” ê²ƒì€ ì˜¤ë²„í‚¬ì¼ ìˆ˜ ìˆìœ¼ë©°, ì˜¤íˆë ¤ ì„±ëŠ¥ì„ ì €í•˜ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ê²°êµ­, ê¸°ìˆ  ì„ íƒì€ í•­ìƒ ë¬¸ì œì˜ íŠ¹ì„±ì— ë§ì¶”ì–´ ê²°ì •ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. FizzBuzzì™€ ê°™ì€ CPU ë°”ìš´ë“œ ì‘ì—…ì—ì„œëŠ” ì „í†µì ì¸ ë™ê¸° ì²˜ë¦¬ ë°©ì‹ì´ ì„±ëŠ¥ë©´ì—ì„œ ë” íš¨ìœ¨ì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°ì€ ì£¼ë¡œ I/O ë°”ìš´ë“œ ì‘ì—…ì—ì„œ ê·¸ ì¥ì ì„ ë°œíœ˜í•˜ë¯€ë¡œ, ì‘ì—…ì˜ ì„±ê²©ì„ ì •í™•íˆ ì´í•´í•˜ê³  ì ì ˆí•œ ë„êµ¬ë¥¼ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.

### íŒŒì´ì¬ì˜ ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬

- í”„ë¡œê·¸ë¨ ì‹¤í–‰

  - ì†ŒìŠ¤ ì½”ë“œ í•´ì„: íŒŒì´ì¬ í”„ë¡œê·¸ë¨ì´ ì‹œì‘ë˜ë©´, íŒŒì´ì¬ ì¸í„°í”„ë¦¬í„°ëŠ” ë¨¼ì € ì†ŒìŠ¤ ì½”ë“œë¥¼ ì½ì–´ ë“¤ì…ë‹ˆë‹¤. ì´ ì½”ë“œëŠ” í…ìŠ¤íŠ¸ íŒŒì¼(.py) í˜•íƒœì¼ ìˆ˜ë„ ìˆê³ , ì´ë¯¸ ì»´íŒŒì¼ëœ ë°”ì´íŠ¸ì½”ë“œ(.pyc)ì¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
  - ì»´íŒŒì¼: ì†ŒìŠ¤ ì½”ë“œëŠ” íŒŒì´ì¬ì˜ ë‚´ì¥ ì»´íŒŒì¼ëŸ¬ì— ì˜í•´ ë°”ì´íŠ¸ì½”ë“œë¡œ ë³€í™˜ë©ë‹ˆë‹¤. ì´ ë°”ì´íŠ¸ì½”ë“œëŠ” íŒŒì´ì¬ ê°€ìƒ ë¨¸ì‹ (PVM)ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ì¤‘ê°„ í˜•íƒœì˜ ì½”ë“œì…ë‹ˆë‹¤.
  - ì‹¤í–‰: ì»´íŒŒì¼ëœ ë°”ì´íŠ¸ì½”ë“œëŠ” íŒŒì´ì¬ ê°€ìƒ ë¨¸ì‹ ì— ì˜í•´ ì‹¤í–‰ë©ë‹ˆë‹¤. PVMì€ ìŠ¤íƒ ê¸°ë°˜ì˜ ì¸í„°í”„ë¦¬í„°ë¡œì„œ, í•˜ë‚˜í•˜ë‚˜ì˜ ì—°ì‚°ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

- ë©”ëª¨ë¦¬ ê´€ë¦¬ ë° ê°€ë¹„ì§€ ì»¬ë ‰ì…˜

  - ìë™ ë©”ëª¨ë¦¬ ê´€ë¦¬: íŒŒì´ì¬ì€ ìë™ ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ê°œë°œìëŠ” ê°ì²´ ìƒì„±ì—ë§Œ ì§‘ì¤‘í•˜ë©´, ìƒì„±ëœ ê°ì²´ì˜ ë©”ëª¨ë¦¬ í• ë‹¹ì€ íŒŒì´ì¬ ì¸í„°í”„ë¦¬í„°ê°€ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
  - ì°¸ì¡° ì¹´ìš´íŒ…: íŒŒì´ì¬ì€ ê°ì²´ì˜ ì°¸ì¡° ìˆ˜ë¥¼ ì¶”ì í•˜ì—¬ ì°¸ì¡° ì¹´ìš´íŠ¸ê°€ 0ì´ ë˜ëŠ” ìˆœê°„ ê°ì²´ë¥¼ ë©”ëª¨ë¦¬ì—ì„œ í•´ì œí•©ë‹ˆë‹¤. ì´ëŠ” ê°€ì¥ ê¸°ë³¸ì ì¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ ë°©ì‹ì…ë‹ˆë‹¤.
  - ê°€ë¹„ì§€ ì»¬ë ‰ì…˜: ì°¸ì¡° ì¹´ìš´íŒ…ë§Œìœ¼ë¡œëŠ” í•´ê²°í•  ìˆ˜ ì—†ëŠ” ìˆœí™˜ ì°¸ì¡° ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ íŒŒì´ì¬ì€ ê°€ë¹„ì§€ ì»¬ë ‰í„°ë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ì£¼ê¸°ì ìœ¼ë¡œ ë©”ëª¨ë¦¬ë¥¼ ê²€ì‚¬í•˜ì—¬ ë„ë‹¬í•  ìˆ˜ ì—†ëŠ” ê°ì²´ë“¤ì„ ì°¾ì•„ ë©”ëª¨ë¦¬ì—ì„œ í•´ì œí•©ë‹ˆë‹¤.
  - ì„¸ëŒ€ë³„ ìˆ˜ì§‘(Generational Collection): íŒŒì´ì¬ì€ ì„¸ëŒ€ë³„ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ëŠ” ê°ì²´ë¥¼ ìƒì„±ëœ ì§€ ì–¼ë§ˆë‚˜ ì˜¤ë˜ë˜ì—ˆëŠ”ì§€ì— ë”°ë¼ ì—¬ëŸ¬ ì„¸ëŒ€ë¡œ ë‚˜ëˆ„ì–´ ê´€ë¦¬í•©ë‹ˆë‹¤. ì Šì€ ì„¸ëŒ€ì˜ ê°ì²´ì—ì„œ ë°œìƒí•˜ëŠ” ê°€ë¹„ì§€ëŠ” ë¹ˆë²ˆí•˜ê²Œ ìˆ˜ì§‘í•˜ê³ , ë‚˜ì´ê°€ ë§ì€ ì„¸ëŒ€ëŠ” ëœ ìì£¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ë©”ëª¨ë¦¬ ê´€ë¦¬ íš¨ìœ¨ì„ ìµœì í™”í•©ë‹ˆë‹¤.

- ì¢…ë£Œ ì²˜ë¦¬
  - ì •ë¦¬ ì‘ì—…: í”„ë¡œê·¸ë¨ì´ ì¢…ë£Œë  ë•Œ, íŒŒì´ì¬ ì¸í„°í”„ë¦¬í„°ëŠ” ì—´ë ¤ ìˆëŠ” íŒŒì¼ í•¸ë“¤ì„ ë‹«ê³ , ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ ì¢…ë£Œí•˜ê³ , í”„ë¡œì„¸ìŠ¤ì— í• ë‹¹ëœ ëª¨ë“  ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.
  - ì¢…ë£Œ ì½”ë“œ ë°˜í™˜: í”„ë¡œê·¸ë¨ì´ ì„±ê³µì ìœ¼ë¡œ ì¢…ë£Œë˜ì—ˆëŠ”ì§€ ë˜ëŠ” ì˜¤ë¥˜ë¡œ ì¸í•´ ì¢…ë£Œë˜ì—ˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì¢…ë£Œ ì½”ë“œë¥¼ ìš´ì˜ ì²´ì œì— ë°˜í™˜í•©ë‹ˆë‹¤.
